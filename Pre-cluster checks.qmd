---
title: "Preclustering data checks"
author: "Alexander Lawless"
date: last-modified
date-format: "DD-MM-YYYY"
title-block-banner: "#f9bf07"
#title-block-banner-color: "#333739"
format:
  html:
    embed-resources: true
    smooth-scroll: true
    theme: cosmo
    fontcolor: black
    toc: true
    toc-location: left
    toc-title: Summary
    toc-depth: 3
#editor: visual
css: styles.css
---

```{r}
#| label: setup
#| #include: TRUE
#| echo: TRUE
#| warning: false
#| code-fold: true
#| code-summary: "expand for setup code chunk"

knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, fig.width =12, fig.height = 9)

library(tidyverse)
library(janitor)

key_measures <-
  read_csv("CSDS_Cluster_analysis___key_measures.csv") |>
  clean_names() |>
  mutate(
    team_input_count = as.numeric(team_input_count),
    intermittent_care_periods = as.numeric(intermittent_care_periods),
    avg_contact_duration = as.numeric(avg_contact_duration),
    acute_admissions_2223 = as.numeric(acute_admissions_2223)
  )


# Remove date fields and replace NA's where appropriate
key_measures_reduced <-
  key_measures |>
  select(-c(first_contact, last_contact, care_mid_date)) |>
  mutate(
    intermittent_care_periods = replace_na(intermittent_care_periods, 0),
    acute_admissions_2223 = replace_na(acute_admissions_2223, 0)
    ) |> #2,120,821
  anti_join(
    key_measures |>
      group_by(person_id) |>
      mutate(rn = row_number()) |> # remove any duplicate person_id rows - just in case
      filter(rn > 1),
    by = "person_id"
    ) # 2,120,817


# Clean names for graphs
key_measure_lookup <-
  tribble(
    ~variable, ~variable_clean,
    "person_id"                 , "0. Person ID",
    "care_contacts"             , "A. Care contacts",
    "referrals"                 , "B. Referrals",
    "length_care"               , "C. Care length",
    "average_daily_contacts"    , "D. Average daily contacts",
    "contact_prop_aft_midpoint" , "E. Contact proportion after midpoint",
    "team_input_count"          , "F. Team input count",
    "intermittent_care_periods" , "G. Intermittent care periods",
    "specialist_contact_prop"   , "H. Specialist care proportion",
    "remote_contact_prop"       , "I. Remote care proportion",
    "avg_contact_duration"      , "J. Average contact duration",
    "acute_admissions_2223"     , "K. Acute admission count"
  )

# Read in imputed key measures

key_measures_reduced_imp <- read_csv("key_measures_reduced_imp.csv")

```

# Pre-cluster checks

## Data summary

```{r}
#| echo: TRUE

skimr::skim(key_measures_reduced)
```

## 1. Check for Missing Values

```{r}
#| echo: TRUE
sum(is.na(key_measures_reduced))
```

Impute values for `team_input_count` and `avg_contact_duration`.

```{r}
#| echo: TRUE
#| eval: FALSE

# Calculate the probability distribution
## Team input
prob_distribution_team <-
  key_measures_reduced |>
  group_by(org_id_provider, team_input_count) |>
  summarise(n = n()) |>
  filter(!is.na(team_input_count)) |>
  mutate(probability = n / sum(n),
         team_count = max(team_input_count)) |>
  rename(field = team_input_count)

## Overall probability distribution for team input (ungrouped by org_id)
overall_prob_distribution_team <-
  key_measures_reduced |>
  group_by(team_input_count) |>
  summarise(n = n()) |>
  filter(!is.na(team_input_count)) |>
  mutate(probability = n / sum(n)) |>
  rename(field = team_input_count)

## Avg contact duration
prob_distribution_duration <-
  key_measures_reduced |>
  group_by(avg_contact_duration) |>
  summarise(n = n()) |>
  filter(!is.na(avg_contact_duration)) |>
  mutate(probability = n / sum(n)) |>
  rename(field = 1)

# Function to impute missing values for avg_contact_duration
impute_avg_contact_duration <- function(series, distribution_df) {
  na_indices <- which(is.na(series))
  sampled_values <- sample(distribution_df$field, size = length(na_indices), replace = TRUE, prob = distribution_df$probability)
  series[na_indices] <- sampled_values
  return(series)
}

# Function to impute missing values for team_input_count
impute_team_input_count <- function(series, distribution_df, overall_distribution_df, org_id_provider) {
  na_indices <- which(is.na(series))
  sampled_values <- vector("numeric", length(na_indices))
  
  for (i in seq_along(na_indices)) {
    org_id <- org_id_provider[na_indices[i]]
    subset_df <- distribution_df[distribution_df$org_id_provider == org_id, ]
    
    sampled_values[i] <- if (nrow(subset_df) > 1) {
      sample(subset_df$field, size = 1, replace = TRUE, prob = subset_df$probability)
    } else if (nrow(subset_df) == 1) {
      subset_df$field
    } else {
      sample(overall_distribution_df$field, size = 1, replace = TRUE, prob = overall_distribution_df$probability)
    }
  }
  
  series[na_indices] <- sampled_values
  return(series)
}

# Impute NA values
key_measures_reduced_imp <-
  key_measures_reduced |>
  mutate(
    team_input_count = impute_team_input_count(team_input_count, prob_distribution_team, overall_prob_distribution_team, org_id_provider),
    avg_contact_duration = impute_avg_contact_duration(avg_contact_duration, prob_distribution_duration)
  )
```

Test imputed data for NA's:

```{r}
#| echo: TRUE

sum(is.na(key_measures_reduced_imp))
```

```{r}
#| echo: TRUE

skimr::skim(key_measures_reduced_imp)
```

## 2. Examine Data Distribution

```{r}
#| echo: FALSE

key_measures_reduced_imp |>
  select(-org_id_provider) |>
  pivot_longer(cols = -person_id) |>
  left_join(key_measure_lookup, by = c("name" = "variable")) |>

  ggplot(aes(x = value)) +
  geom_density() +
  facet_wrap(~variable_clean, scales = "free") +
  theme_minimal() +
  theme(
    strip.background = element_rect(fill = NA, colour = "grey"),
    strip.text = element_text(face = "bold")
    ) +
  labs(title = "Distribution of key measures",
       subtitle = "CSDS Pre-cluster analysis assessment")
```

**Do I need to transform skewed variables?**

## 3. Outlier Detection

```{r}
#| echo: TRUE

remove_outliers <-
  key_measures_reduced_imp |>
  select(-org_id_provider) |>
  pivot_longer(cols = -person_id) |>
  group_by(name) |>
  mutate(quantile_99 = quantile(value, 0.99)) |>
  ungroup() |>
  filter(value > quantile_99) |>  # remove values above 99th percentile
  select(person_id) |>
  distinct()

key_measures_reduced_imp_ex_outlier <-
  key_measures_reduced_imp |>
  select(-org_id_provider) |>
  anti_join(remove_outliers, by = "person_id")
```

Plot distrubution with outliers removed:

```{r}
#| echo: FALSE

key_measures_reduced_imp_ex_outlier |>
  pivot_longer(cols = -person_id) |>
  left_join(key_measure_lookup, by = c("name" = "variable")) |>
  ggplot(aes(x = value)) +
  geom_density() +
  facet_wrap(~variable_clean, scales = "free") +
  theme_minimal() +
  theme(
    strip.background = element_rect(fill = NA, colour = "grey"),
    strip.text = element_text(face = "bold")
  ) +
  labs(title = "Distribution of key measures",
       subtitle = "CSDS Pre-cluster analysis assessment")
```

## 4. Correlation Analysis

```{r}
#| echo: TRUE

cor_matrix_2 <- 
  cor(
    key_measures_reduced_imp_ex_outlier |>
      pivot_longer(-person_id) |>
      left_join(key_measure_lookup, by = c("name" = "variable")) |>
      select(-name) |>
      pivot_wider(id_cols = "person_id",
                  names_from = "variable_clean",
                  values_from = "value") |>
      select(-person_id)
    )  # Exclude person_id

corrplot::corrplot(
  cor_matrix_2,
  method = "square",
  type = "lower",
  tl.col = "black"
)

```

## 5. Standardise/Normalise Data 

```{r}
#| echo: TRUE

key_measures_scaled <- scale(key_measures_reduced_imp_ex_outlier[, -1])  # Exclude person_id

key_measures_reduced_imp_ex_outlier |>
  pivot_longer(-person_id) |>
  group_by(name) |>
  mutate(value_scale = scale(value)) |>
  left_join(key_measure_lookup, by = c("name" = "variable")) |>

  ggplot(aes(x = value_scale)) +
  geom_density() +
  facet_wrap(~variable_clean, scales = "free") +
  theme_minimal() +
  theme(
    strip.background = element_rect(fill = NA, colour = "grey"),
    strip.text = element_text(face = "bold")
  ) +
  labs(title = "Distribution of key measures - scaled",
       subtitle = "CSDS Pre-cluster analysis assessment")
```

## 6. Assess Multicollinearity

```{r}
#| echo: TRUE

car::vif(lm(care_contacts ~ ., data = key_measures_reduced_imp_ex_outlier[, -1]))  # Example using VIF
```

A Variance Inflation Factor (VIF) indicates the degree to which a predictor variable in a regression model is correlated with other predictor variables.

Interpretation:

  * A value of **1 signifying no correlation** and higher values representing increasing levels of multicollinearity, 
  * A VIF **between 1 and 5** is considered **moderately correlated**, 
  * Values **above 5** suggest potentially problematic multicollinearity that might require **further investigation or corrective actions** like removing or combining highly correlated variables.

## 7. Dimensionality Reduction
Using PCA (Principal Component Analysis) to reduce dimensionality.

```{r}
#| echo: TRUE

pca_result <- prcomp(key_measures_scaled, center = TRUE, scale. = TRUE)
summary(pca_result)
```


Visualise the progressive proportion of variance explained by each variable:
```{r}
#| echo: TRUE

screeplot(pca_result, type = "lines")
```

If PC1 explains 23.82% of the variance and PC2 explains 12.84%, together they explain 36.66% of the variance. This means that the first two principal components capture a significant portion of the information in the dataset.

Additional visualisation:
```{r}
#| echo: TRUE

#biplot(pca_result)
```

